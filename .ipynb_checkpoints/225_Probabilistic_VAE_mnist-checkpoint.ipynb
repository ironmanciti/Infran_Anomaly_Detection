{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltPJCG6pAUoc"
   },
   "source": [
    "# TFP Probabilistic Layers: Variational Auto Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRVR-tGTR31S"
   },
   "source": [
    "2019 년 발표된 TensorFlow Probability(TFP) 레이어를 사용하여 간단하게 VAE(Variational Autoencoder)를 만듭니다.\n",
    "\n",
    "TFP 레이어는 Keras를 사용하여 심층 네트워크로 배포를 구성하기 위한 high-level API를 제공합니다. 이 API를 사용하면 딥러닝과 확률 프로그래밍을 결합한 모델을 쉽게 구축할 수 있습니다. 예를 들어, 심층 네트워크의 출력으로 확률 분포를 매개변수화할 수 있습니다.   \n",
    "\n",
    "### Variational Autoencoder와 ELBO\n",
    "VAE(Variational Autoencoder)는 collaborative filtering, 이미지 압축, 강화 학습, 음악 및 스케치 생성 등 다양한 영역에서 사용되는 인기 있는 생성 모델입니다.  \n",
    "\n",
    "MNIST 같은 숫자를 그리는 과정에서 우리는 잠재 변수 생성 모델과 같이 데이터를 생성하는 일부 프로세스를 머리 속으로 상상합니다. 숫자를 그리기 전에 먼저 그릴 숫자를 결정하고 머릿속에 흐릿한 그림을 상상한다고 가정해 보겠습니다. 그런 다음 종이에 펜을 대고 실제 세계에서 그림을 만들어 봅니다. 이 두 단계 프로세스를  다음과 같이 공식화할 수 있습니다.  \n",
    "\n",
    "1. 어떤 사전 분포 z ~ p(z)에서 어떤 잠재 표현 z를 샘플링합니다. 이것은 당신의 머리에 있는 흐릿한 그림입니다. \"3\"이라고 가정해 봅시다.  \n",
    "2. 샘플을 기반으로 확률적 프로세스 x ~ p(x|z)로 모델링된 실제 그림 표현 x를 그립니다. 이렇게 하면 \"3\"을 쓸 때마다 조금씩 다르게 보입니다.  \n",
    "\n",
    "손으로 글자를 쓸 때 같은 사람이 그려도 조금씩 달라지는 원인의 일부는 MNIST 숫자의 클래스 고유의 신호에 기인하고 다른 일부는 노이즈로 인한 것입니다. VAE는 두 프로세스의 명시적 모델을 사용하여 노이즈에서 신호를 분리하려는 시도입니다.  \n",
    "\n",
    "이 목표를 훈련하기 위해 ELBO(Evidence Lower BOund) 목적함수를 최대화합니다.  \n",
    "\n",
    "$$ELBO(x)=\\int{dz}q(z|x)log{p(x|z)} + \\int{dz}q(z|x)log{\\frac{q(z|x)}{p(z)}}$$\n",
    "\n",
    "여기서 3 가지 확률 밀도 함수는;\n",
    "- $p(z)$ - 잠재 표현 z 의 prior  \n",
    "- $q(z|x)$ - variational encoder  \n",
    "- $p(x|z)$ - decoder (잠재 표현 z가 주어질 때 image x 의 가능도)  \n",
    "\n",
    "ELBO는 관찰된 데이터 포인트의 로그 확률인 $log{p(x)}$의 하한입니다. ELBO 방정식의 첫 번째 적분은 재구성 항입니다. 그것은 우리가 이미지 x에서 시작하여 z로 인코딩하고 디코딩하고 원래 x를 다시 가져올 가능성을 묻습니다. 두 번째 항은 KL Divergence 항입니다. 인코더와 prior가 얼마나 가까운지를 측정합니다. 이 항은 인코더를 정직하게 유지하려는 것으로 생각할 수 있습니다. 인코더가 prior에 전혀 있을 것 같지 않은 z개의 샘플을 생성하는 경우 object는 prior와 비슷한 z개의 샘플을 생성하는 경우보다 나쁩니다. 따라서 인코더는 비용이 재구성 항의 이점보다 더 큰 경우에만 prior와 달라야 합니다.\n",
    "\n",
    "### Code\n",
    "위의 설명에 따라 세 가지 다른 구성 요소를 개별적으로 모델링하는 것이 타당하다는 것을 알 수 있습니다.   \n",
    "\n",
    "Pior - $p(z)$   \n",
    "Autoencoder - $q(z|x)$  \n",
    "Decoder - $p(x|z)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "kZ0MdF1j8WJf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow_probability as tfp\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import InputLayer, Conv2D, Flatten, Dense, \\\n",
    "                                                         Lambda, Reshape, Conv2DTranspose\n",
    "tfd = tfp.distributions\n",
    "tfpl = tfp.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N8Shtn_e99XC"
   },
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "daPl6ycN9cD3"
   },
   "outputs": [],
   "source": [
    "datasets, datasets_info = tfds.load(name='mnist',\n",
    "                                    with_info=True,\n",
    "                                    as_supervised=False)\n",
    "\n",
    "def _preprocess(sample):\n",
    "    image = tf.cast(sample['image'], tf.float32) / 255.  # Scale to unit interval.\n",
    "    image = image < tf.random.uniform(tf.shape(image))   # Randomly binarize.\n",
    "    return image, image\n",
    "\n",
    "train_dataset = (datasets['train']\n",
    "                 .map(_preprocess)\n",
    "                 .batch(256)\n",
    "                 .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "                 .shuffle(int(10e3)))\n",
    "test_dataset = (datasets['test']\n",
    "                .map(_preprocess)\n",
    "                .batch(256)\n",
    "                .prefetch(tf.data.experimental.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPKblOe58Hql"
   },
   "source": [
    "위의 _preprocess()는 Keras가 (example, label) 입력 형식 (예 : $p_\\theta(y|x)$)을 사용하는 discriminative model 용으로 설정되었기 때문에 `image`가 아닌 `image, image`를 반환합니다. VAE의 목표는 x 자체 (예 : $p_\\theta(x|x)$)에서 입력 x를 복구하는 것이므로 데이터 쌍은 (example, example)입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cGaLmdt7GvVG",
    "outputId": "d2a56bc2-cad0-4a39-e0c2-08a54330b94f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<ShuffleDataset element_spec=(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.bool, name=None), TensorSpec(shape=(None, 28, 28, 1), dtype=tf.bool, name=None))>,\n",
       " <PrefetchDataset element_spec=(TensorSpec(shape=(None, 28, 28, 1), dtype=tf.bool, name=None), TensorSpec(shape=(None, 28, 28, 1), dtype=tf.bool, name=None))>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CI-VFyp8-BIa"
   },
   "source": [
    "### VAE Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKgRI5eoS2rx"
   },
   "source": [
    "#### Specify model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "rd3Voa64_Gtv"
   },
   "outputs": [],
   "source": [
    "input_shape = datasets_info.features['image'].shape\n",
    "base_depth = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TfsLCsjTE-BB"
   },
   "source": [
    "### Prior - latent representation z 은 isotropic Gaussian (등방성 가우시안 분포) 로 가정 \n",
    "\n",
    "- Covariance가 identity matrix의 scala 배"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "9d7Jbm66FN_u"
   },
   "outputs": [],
   "source": [
    "encoded_size = 16\n",
    "prior = tfd.Independent(tfd.Normal(loc=tf.zeros(encoded_size), scale=1),\n",
    "                                     reinterpreted_batch_ndims=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ind = tfd.Independent(\n",
    "#     distribution=tfd.Normal(loc=tf.zeros(encoded_size), scale=1),\n",
    "#     reinterpreted_batch_ndims=1)\n",
    "\n",
    "# print(ind.mean(), ind.stddev())\n",
    "\n",
    "# # 2d 구형 Gaussian 의 approximate density contour 를 plot\n",
    "# N = 1000\n",
    "# x = ind.sample(N)\n",
    "# print(x.shape)\n",
    "\n",
    "# x1 = x[:, 0]\n",
    "# x2 = x[:, 1]\n",
    "# sns.jointplot(x=x1, y=x2, kind = 'kde', space = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mu = [0., 0.]  # mean\n",
    "# scale_tril = [[1.,  0.], [0.6, 0.8]]  # covariance\n",
    "# tril = tfd.MultivariateNormalTriL(loc = mu, scale_tril = scale_tril)\n",
    "# print(tril.mean(), tril.covariance())\n",
    "\n",
    "# x = tril.sample(N)\n",
    "# x1 = x[:, 0]\n",
    "# x2 = x[:, 1]\n",
    "# sns.jointplot(x=x1, y=x2, kind = 'kde', space = 0, color = 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2oMVyDMTGJ-e"
   },
   "source": [
    "## Encoder\n",
    "인코더 분포의 경우 신경망의 출력으로 매개 변수화된 평균 및 공분산 행렬과 함께 완전 공분산 가우스 분포를 사용합니다. 복잡하게 들릴 수 있지만 TFP 레이어로 표현하는 것은 매우 쉽습니다.  \n",
    "\n",
    "인코더는 컨볼루션과 Dense 레이어로 구성된 일반적인 Keras Sequential 모델일 뿐 이지만 출력은 TFP 레이어인 MultivariateNormalTril()로 전달되어 최종 Dense() 레이어의 activaton을 mean 과 Multivariate Normal의 parameter인 (하삼각형) covariance matrix를 둘 다 지정하는데 필요한 부분으로 명확히 분할합니다.   \n",
    "`tfpl.MultivariateNormalTriL.params_size(encoded_size)`를 Dense() layer output을 정확히 만들기 위해 사용합니다.\n",
    "\n",
    "마지막으로 우리는 분포가 최종 손실에 \"정규화\" 항을 제공해야 한다고 말했습니다. 특히, encoder와 loss 에 대한 prior 사이에 KL Divergence를 추가합니다. ELBO의 KL 항 입니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "eRHjRtAL-e33"
   },
   "outputs": [],
   "source": [
    "encoder = tf.keras.Sequential([\n",
    "    InputLayer(input_shape=input_shape),\n",
    "    Lambda(lambda x: tf.cast(x, tf.float32) - 0.5),\n",
    "    Conv2D(base_depth, 5, strides=1,\n",
    "                padding='same', activation=tf.nn.leaky_relu),\n",
    "    Conv2D(base_depth, 5, strides=2,\n",
    "                padding='same', activation=tf.nn.leaky_relu),\n",
    "    Conv2D(2 * base_depth, 5, strides=1,\n",
    "                padding='same', activation=tf.nn.leaky_relu),\n",
    "    Conv2D(2 * base_depth, 5, strides=2,\n",
    "                padding='same', activation=tf.nn.leaky_relu),\n",
    "    Conv2D(4 * encoded_size, 7, strides=1,\n",
    "                padding='valid', activation=tf.nn.leaky_relu),\n",
    "    Flatten(),\n",
    "    Dense(tfpl.MultivariateNormalTriL.params_size(encoded_size),\n",
    "               activation=None),  \n",
    "    tfpl.MultivariateNormalTriL(\n",
    "        encoded_size,\n",
    "        activity_regularizer=tfpl.KLDivergenceRegularizer(prior)),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DgSLVvTsnUfS",
    "outputId": "896cbda2-5d62-4a2c-89db-246784b38acd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfpl.MultivariateNormalTriL.params_size(encoded_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JWHNRQaDHJ6_"
   },
   "source": [
    "## Decoder\n",
    "단순한 `mean-field-decoder`를 사용합니다. 이 경우 pixel-dependent한 Bernoulli 분포 입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "baP--pt6-ewK"
   },
   "outputs": [],
   "source": [
    "decoder = tf.keras.Sequential([\n",
    "    InputLayer(input_shape=[encoded_size]),\n",
    "    Reshape([1, 1, encoded_size]),\n",
    "    Conv2DTranspose(2 * base_depth, 7, strides=1,\n",
    "                         padding='valid', activation=tf.nn.leaky_relu),\n",
    "    Conv2DTranspose(2 * base_depth, 5, strides=1,\n",
    "                         padding='same', activation=tf.nn.leaky_relu),\n",
    "    Conv2DTranspose(2 * base_depth, 5, strides=2,\n",
    "                         padding='same', activation=tf.nn.leaky_relu),\n",
    "    Conv2DTranspose(base_depth, 5, strides=1,\n",
    "                         padding='same', activation=tf.nn.leaky_relu),\n",
    "    Conv2DTranspose(base_depth, 5, strides=2,\n",
    "                         padding='same', activation=tf.nn.leaky_relu),\n",
    "    Conv2DTranspose(base_depth, 5, strides=1,\n",
    "                         padding='same', activation=tf.nn.leaky_relu),\n",
    "    Conv2D(filters=1, kernel_size=5, strides=1,\n",
    "                padding='same', activation=None),\n",
    "    Flatten(),\n",
    "    tfpl.IndependentBernoulli(input_shape, tfd.Bernoulli.logits),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "7itugvZVLyWL"
   },
   "outputs": [],
   "source": [
    "vae = tf.keras.Model(inputs=encoder.inputs,\n",
    "                outputs=decoder(encoder.outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ckYuzfILkVb"
   },
   "source": [
    "## Loss\n",
    "\n",
    "이미 KL 항이 loss 에 더해졌으므로 reconstruction error 만 지정하면 됩니다. (ELBO 의 첫번째 항)\n",
    "\n",
    "loss function 은 original input x 와 model 의 output 두개의 인수를 받습니다. random variable이므로 rv_x로 부릅니다. keras와 tensorflow가 TFP layer를 출력 tensor로 보더라도, TFP layer는 실제로는 Distribution object 입니다. 따라서, loss function을 주어진 model에서 data 의 negative log likelihood (-rv_x.log_prob(x))로 만들 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e7f1u-Ya-axQ",
    "outputId": "15e362a8-1c28-40bd-b5f1-de99c10e7af0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "  9/235 [>.............................] - ETA: 3:28 - loss: 468.0091"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [15]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m negative_log_likelihood \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x, rv_x: \u001b[38;5;241m-\u001b[39mrv_x\u001b[38;5;241m.\u001b[39mlog_prob(x)\n\u001b[0;32m      3\u001b[0m vae\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m),\n\u001b[0;32m      4\u001b[0m             loss\u001b[38;5;241m=\u001b[39mnegative_log_likelihood)\n\u001b[1;32m----> 6\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf20\\lib\\site-packages\\keras\\utils\\traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 64\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf20\\lib\\site-packages\\keras\\engine\\training.py:1409\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1402\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1403\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   1404\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   1405\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[0;32m   1406\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   1407\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m   1408\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1409\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1410\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1411\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf20\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf20\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf20\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf20\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2453\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2451\u001b[0m   (graph_function,\n\u001b[0;32m   2452\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf20\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1860\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1856\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1858\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1859\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1860\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1861\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1862\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1863\u001b[0m     args,\n\u001b[0;32m   1864\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1865\u001b[0m     executing_eagerly)\n\u001b[0;32m   1866\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf20\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:497\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    496\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    503\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    504\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    505\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    506\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    509\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    510\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tf20\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "negative_log_likelihood = lambda x, rv_x: -rv_x.log_prob(x)\n",
    "\n",
    "vae.compile(optimizer=tf.optimizers.Adam(learning_rate=1e-3),\n",
    "            loss=negative_log_likelihood)\n",
    "\n",
    "_ = vae.fit(train_dataset,\n",
    "            epochs=30,\n",
    "            validation_data=test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8phRwAd-jwv6"
   },
   "source": [
    "## original image 를 vae network 을 통과시켜 얻은 distribution object 로 image sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3ZqfOYMP_2p_",
    "outputId": "a8902791-c3a7-4fc7-da00-cff5a52ac3eb"
   },
   "outputs": [],
   "source": [
    "#무작위 숫자 10개만 조사하겠습니다.\n",
    "x = next(iter(test_dataset))[0][:10]\n",
    "xhat = vae(x)\n",
    "print(x.shape)\n",
    "assert isinstance(xhat, tfd.Distribution)\n",
    "print(xhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkvqEcRYac4R"
   },
   "source": [
    "시각화 도우미 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MM7wW4S2OrBt"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def display_imgs(x, y=None):\n",
    "    if not isinstance(x, (np.ndarray, np.generic)):\n",
    "        x = np.array(x)\n",
    "    n = x.shape[0]\n",
    "    fig, axs = plt.subplots(1, n, figsize=(n, 1))\n",
    "    if y is not None:\n",
    "        fig.suptitle(np.argmax(y, axis=1))\n",
    "    for i in range(n):\n",
    "        axs.flat[i].imshow(x[i].squeeze(), interpolation='none', cmap='gray')\n",
    "        axs.flat[i].axis('off')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    plt.ion()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z7wyT8tLiCrS",
    "outputId": "12f712ad-b4fa-4c8c-be0e-2a8a8dddd9e7"
   },
   "outputs": [],
   "source": [
    "xhat.sample().shape, xhat.mode().shape, xhat.mean().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337
    },
    "id": "ow7rfh6YLLx1",
    "outputId": "b187fd8a-26af-42e8-dbff-a06543e288a4"
   },
   "outputs": [],
   "source": [
    "print('무작위로 고른 Original Image:')\n",
    "display_imgs(x)\n",
    "\n",
    "print('Decoded 분포에서 무작위 sample:')\n",
    "display_imgs(xhat.sample())\n",
    "\n",
    "print('Decoded 분포의 최빈값:')\n",
    "display_imgs(xhat.mode())\n",
    "\n",
    "print('Decoded 분포의 평균값:')\n",
    "display_imgs(xhat.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72LqsAD9jFaJ"
   },
   "source": [
    "## original image 가 아니라 prior 에서 sampling 한 latent variable 로 image 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C3_5HPUCQpYO",
    "outputId": "10b9a37d-4315-4e6d-e88e-576ce044ffd8"
   },
   "outputs": [],
   "source": [
    "z = prior.sample(10)\n",
    "xtilde = decoder(z)\n",
    "print(xtilde)\n",
    "assert isinstance(xtilde, tfd.Distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 257
    },
    "id": "_jMPwz8r9pYX",
    "outputId": "afeb87e5-2407-4ce3-c41f-a10057bb49ed"
   },
   "outputs": [],
   "source": [
    "print('Randomly Generated Samples:')\n",
    "display_imgs(xtilde.sample())\n",
    "\n",
    "print('Randomly Generated Modes:')\n",
    "display_imgs(xtilde.mode())\n",
    "\n",
    "print('Randomly Generated Means:')\n",
    "display_imgs(xtilde.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m5QJVGFIppk2"
   },
   "source": [
    "###  latent variable 시각화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k7f_fvSYmV3j"
   },
   "outputs": [],
   "source": [
    "z = encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TKlm3N-MDK9u",
    "outputId": "75ebf7fc-4dda-4e2f-882b-0e80c020c3ac"
   },
   "outputs": [],
   "source": [
    "latent = z.sample()\n",
    "latent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 78
    },
    "id": "1ZbKFwoWn9nd",
    "outputId": "016a82d6-db1d-427e-cd22-58a252fc54d9"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 10, figsize=(10, 1))\n",
    "plt.ioff()\n",
    "for i in range(10):\n",
    "    axs.flat[i].imshow(latent[i].numpy().reshape(4, 4), interpolation='none', cmap='gray')\n",
    "    axs.flat[i].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fmcgm2uib5_7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "225_Probabilistic_VAE_mnist.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
