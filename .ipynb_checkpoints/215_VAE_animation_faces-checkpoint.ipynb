{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "prTKL3d2kGZE"
   },
   "source": [
    "# Variational Autoencoders on Anime Faces\n",
    "\n",
    "이 연습에서는 [MckInsey666의 애니메이션 얼굴 데이터 세트](https://github.com/bchao1/Anime-Face-Dataset)를 사용하여 VAE (Variational Autoencoder)를 학습합니다.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MooRFGEeI1zb"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import random\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pjhN6GgfmUfx"
   },
   "outputs": [],
   "source": [
    "np.random.seed(51)\n",
    "\n",
    "BATCH_SIZE=2000\n",
    "LATENT_DIM=512\n",
    "IMAGE_SIZE=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qxKW6Q88KHcL"
   },
   "outputs": [],
   "source": [
    "# make the data directory\n",
    "try:\n",
    "  os.mkdir('/tmp/anime')\n",
    "except OSError:\n",
    "  pass\n",
    "\n",
    "# download the zipped dataset to the data directory\n",
    "data_url = \"https://storage.googleapis.com/laurencemoroney-blog.appspot.com/Resources/anime-faces.zip\"\n",
    "data_file_name = \"animefaces.zip\"\n",
    "download_dir = '/tmp/anime/'\n",
    "urllib.request.urlretrieve(data_url, data_file_name)\n",
    "\n",
    "# extract the zip file\n",
    "zip_ref = zipfile.ZipFile(data_file_name, 'r')\n",
    "zip_ref.extractall(download_dir)\n",
    "zip_ref.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NTlx97U_JDPB"
   },
   "outputs": [],
   "source": [
    "# Data Preparation Utilities\n",
    "\n",
    "def get_dataset_slice_paths(image_dir):\n",
    "  '''returns a list of paths to the image files'''\n",
    "  image_file_list = os.listdir(image_dir)\n",
    "  image_paths = [os.path.join(image_dir, fname) for fname in image_file_list]\n",
    "\n",
    "  return image_paths\n",
    "\n",
    "def map_image(image_filename):\n",
    "  '''preprocesses the images'''\n",
    "  img_raw = tf.io.read_file(image_filename)\n",
    "  image = tf.image.decode_jpeg(img_raw)\n",
    "\n",
    "  image = tf.cast(image, dtype=tf.float32)\n",
    "  image = tf.image.resize(image, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "  image = image / 255.0  \n",
    "  image = tf.reshape(image, shape=(IMAGE_SIZE, IMAGE_SIZE, 3,))\n",
    "\n",
    "  return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iGoCJ6DPJHL8"
   },
   "outputs": [],
   "source": [
    "# get the list containing the image paths\n",
    "paths = get_dataset_slice_paths(\"/tmp/anime/images/\")\n",
    "\n",
    "# shuffle the paths\n",
    "random.shuffle(paths)\n",
    "\n",
    "# split the paths list into to training (80%) and validation sets(20%).\n",
    "paths_len = len(paths)\n",
    "train_paths_len = int(paths_len * 0.8)\n",
    "\n",
    "train_paths = paths[:train_paths_len]\n",
    "val_paths = paths[train_paths_len:]\n",
    "\n",
    "# load the training image paths into tensors, create batches and shuffle\n",
    "training_dataset = tf.data.Dataset.from_tensor_slices((train_paths))\n",
    "training_dataset = training_dataset.map(map_image)\n",
    "training_dataset = training_dataset.shuffle(1000).batch(BATCH_SIZE)\n",
    "\n",
    "# load the validation image paths into tensors and create batches\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((val_paths))\n",
    "validation_dataset = validation_dataset.map(map_image)\n",
    "validation_dataset = validation_dataset.batch(BATCH_SIZE)\n",
    "\n",
    "\n",
    "print(f'number of batches in the training set: {len(training_dataset)}')\n",
    "print(f'number of batches in the validation set: {len(validation_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "72ZRga9vlonx"
   },
   "source": [
    "## Display Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jC1cpLViJLIu"
   },
   "outputs": [],
   "source": [
    "def display_faces(dataset, size=9):\n",
    "  '''Takes a sample from a dataset batch and plots it in a grid.'''\n",
    "  dataset = dataset.unbatch().take(size)\n",
    "  n_cols = 3\n",
    "  n_rows = size//n_cols + 1\n",
    "  plt.figure(figsize=(5, 5))\n",
    "  i = 0\n",
    "  for image in dataset:\n",
    "    i += 1\n",
    "    disp_img = np.reshape(image, (64,64,3))\n",
    "    plt.subplot(n_rows, n_cols, i)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(disp_img)\n",
    "\n",
    "\n",
    "def display_one_row(disp_images, offset, shape=(28, 28)):\n",
    "  '''Displays a row of images.'''\n",
    "  for idx, image in enumerate(disp_images):\n",
    "    plt.subplot(3, 10, offset + idx + 1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    image = np.reshape(image, shape)\n",
    "    plt.imshow(image)\n",
    "\n",
    "\n",
    "def display_results(disp_input_images, disp_predicted):\n",
    "  '''Displays input and predicted images.'''\n",
    "  plt.figure(figsize=(15, 5))\n",
    "  display_one_row(disp_input_images, 0, shape=(IMAGE_SIZE,IMAGE_SIZE,3))\n",
    "  display_one_row(disp_predicted, 20, shape=(IMAGE_SIZE,IMAGE_SIZE,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5eZsrZtqJOzv"
   },
   "outputs": [],
   "source": [
    "display_faces(validation_dataset, size=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LSBtdCVim9aC"
   },
   "source": [
    "## Build the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQvzWaNqLrB1"
   },
   "source": [
    "You will be building your VAE in the following sections. Recall that this will follow and encoder-decoder architecture and can be summarized by the figure below.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1YAZAeMGEJ1KgieYk1ju-S9DoshpMREeC\" width=\"60%\" height=\"60%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KHNxIUUS9ng9"
   },
   "source": [
    "### Sampling Class\n",
    "\n",
    "You will start with the custom layer to provide the Gaussian noise input along with the mean (mu) and standard deviation (sigma) of the encoder's output. Recall the equation to combine these:\n",
    "\n",
    "$$z = \\mu + e^{0.5\\sigma} * \\epsilon  $$\n",
    "\n",
    "where $\\mu$ = mean, $\\sigma$ = standard deviation, and $\\epsilon$ = random sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L-3qk6ZBm0Fl"
   },
   "outputs": [],
   "source": [
    "class Sampling(tf.keras.layers.Layer):\n",
    "  def call(self, inputs):\n",
    "    \"\"\"Generates a random sample and combines with the encoder output\n",
    "    \n",
    "    Args:\n",
    "      inputs -- output tensor from the encoder\n",
    "\n",
    "    Returns:\n",
    "      `inputs` tensors combined with a random sample\n",
    "    \"\"\"\n",
    "    mu, sigma = inputs\n",
    "    batch = tf.shape(mu)[0]\n",
    "    dim = tf.shape(mu)[1]\n",
    "    epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "    z = mu + tf.exp(0.5 * sigma) * epsilon\n",
    "    return  z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tZjCSa7Y-Gvk"
   },
   "source": [
    "### Encoder Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7VSVYjDim4Dk"
   },
   "outputs": [],
   "source": [
    "def encoder_layers(inputs, latent_dim):\n",
    "  \"\"\"Defines the encoder's layers.\n",
    "  Args:\n",
    "    inputs -- batch from the dataset\n",
    "    latent_dim -- dimensionality of the latent space\n",
    "\n",
    "  Returns:\n",
    "    mu -- learned mean\n",
    "    sigma -- learned standard deviation\n",
    "    batch_3.shape -- shape of the features before flattening\n",
    "  \"\"\"\n",
    "  x = tf.keras.layers.Conv2D(32, (3,3), strides=2, padding='same', activation='relu')(inputs)\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "  x = tf.keras.layers.Conv2D(64, (3,3), strides=2, padding='same', activation='relu')(x)\n",
    "  X = tf.keras.layers.BatchNormalization()(x)\n",
    "  x = tf.keras.layers.Conv2D(128, (3,3), strides=2, padding='same', activation='relu')(x)\n",
    "  batch_3 = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  x = tf.keras.layers.Flatten()(batch_3)\n",
    "  x = tf.keras.layers.Dense(1024, activation='relu')(x)\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  mu = tf.keras.layers.Dense(latent_dim)(x)\n",
    "  sigma = tf.keras.layers.Dense(latent_dim)(x)\n",
    "\n",
    "  # revise `batch_3.shape` here if you opted not to use 3 Conv2D layers\n",
    "  return mu, sigma, batch_3.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZOy7wPPY-g-N"
   },
   "source": [
    "### Encoder Model\n",
    "\n",
    "위 함수의 출력을 이전에 정의한 `Sampling layer`로 공급합니다. 나중에 디코더 네트워크에 공급할 수있는 잠재 표현을 갖게됩니다. `Sampling` 레이어로 인코더 네트워크를 구축하려면 아래 기능을 완료하세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w8Y-wLFym60N"
   },
   "outputs": [],
   "source": [
    "def encoder_model(latent_dim, input_shape):\n",
    "  \"\"\"Defines the encoder model with the Sampling layer\n",
    "  Args:\n",
    "    latent_dim -- dimensionality of the latent space\n",
    "    input_shape -- shape of the dataset batch\n",
    "\n",
    "  Returns:\n",
    "    model -- the encoder model\n",
    "    conv_shape -- shape of the features before flattening\n",
    "  \"\"\"\n",
    "  inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "  mu, sigma, conv_shape = encoder_layers(inputs, latent_dim)\n",
    "  z = Sampling()((mu, sigma))\n",
    "  model = tf.keras.models.Model(inputs, outputs=[mu, sigma, z])\n",
    "\n",
    "  model.summary()\n",
    "  return model, conv_shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9ENB-6a-0R5"
   },
   "source": [
    "### Decoder Layers\n",
    "\n",
    "다음으로 디코더 레이어를 정의합니다. 이렇게하면 잠재 표현이 원래 이미지 dimension으로 다시 확장됩니다. VAE 모델을 학습한 후 이 디코더 모델을 사용하여 random input을 제공하여 새 데이터를 생성할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qlTjAzgsm9Vn"
   },
   "outputs": [],
   "source": [
    "def decoder_layers(inputs, conv_shape):\n",
    "  \"\"\"Defines the decoder layers.\n",
    "  Args:\n",
    "    inputs -- output of the encoder \n",
    "    conv_shape -- shape of the features before flattening\n",
    "\n",
    "  Returns:\n",
    "    tensor containing the decoded output\n",
    "  \"\"\"\n",
    "  units = conv_shape[1] * conv_shape[2] * conv_shape[3]\n",
    "  x = tf.keras.layers.Dense(units, activation='relu')(inputs)\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "  # reshape output using the conv_shape dimensions\n",
    "  x = tf.keras.layers.Reshape((conv_shape[1], conv_shape[2], conv_shape[3]))(x)\n",
    "\n",
    "  # upsample the features back to the original dimensions\n",
    "  x = tf.keras.layers.Conv2DTranspose(128, (3,3), strides=2, padding='same', activation='relu')(x)\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "  x = tf.keras.layers.Conv2DTranspose(64, (3,3), strides=2, padding='same', activation='relu')(x)\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "  x = tf.keras.layers.Conv2DTranspose(32, (3,3), strides=2, padding='same', activation='relu')(x)\n",
    "  x = tf.keras.layers.BatchNormalization()(x)\n",
    "  x = tf.keras.layers.Conv2DTranspose(3, (3,3), strides=1, padding='same', activation='sigmoid')(x)\n",
    "  return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cfLLz84r_MlN"
   },
   "source": [
    "### Decoder Model\n",
    "\n",
    "Please complete the function below to output the decoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sUgTyqNFm_jR"
   },
   "outputs": [],
   "source": [
    "def decoder_model(latent_dim, conv_shape):\n",
    "  \"\"\"Defines the decoder model.\n",
    "  Args:\n",
    "    latent_dim -- dimensionality of the latent space\n",
    "    conv_shape -- shape of the features before flattening\n",
    "\n",
    "  Returns:\n",
    "    model -- the decoder model\n",
    "  \"\"\"\n",
    "  inputs = tf.keras.layers.Input(shape=(latent_dim,))\n",
    "  outputs = decoder_layers(inputs, conv_shape)\n",
    "  model = tf.keras.models.Model(inputs, outputs)\n",
    "  model.summary()\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ps0yuE1d_cQc"
   },
   "source": [
    "### Kullback–Leibler Divergence\n",
    "\n",
    "다음으로 [Kullback–Leibler Divergence] (https://arxiv.org/abs/2002.07514) 손실을 계산하는 함수를 정의합니다. 이것은 모델의 생성 능력을 향상시키는 데 사용됩니다. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tngFmDDwnDn-"
   },
   "outputs": [],
   "source": [
    "def kl_reconstruction_loss(inputs, outputs, mu, sigma):\n",
    "  \"\"\" Computes the Kullback-Leibler Divergence (KLD)\n",
    "  Args:\n",
    "    inputs -- batch from the dataset\n",
    "    outputs -- output of the Sampling layer\n",
    "    mu -- mean\n",
    "    sigma -- standard deviation\n",
    "\n",
    "  Returns:\n",
    "    KLD loss\n",
    "  \"\"\"\n",
    "  kl_loss = 1 + sigma - tf.square(mu) - tf.math.exp(sigma)\n",
    "  return tf.reduce_mean(kl_loss) * -0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pi1I431I_og7"
   },
   "source": [
    "### Putting it all together\n",
    "\n",
    "전체 VAE 모델을 정의하십시오. KL 재구성 손실을 추가하려면 `model.add_loss()`를 사용해야 합니다. 이것은 나중에 학습 루프에서 액세스되고 손실에 추가됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cuPHg28JnGCp"
   },
   "outputs": [],
   "source": [
    "def vae_model(encoder, decoder, input_shape):\n",
    "  \"\"\"Defines the VAE model\n",
    "  Args:\n",
    "    encoder -- the encoder model\n",
    "    decoder -- the decoder model\n",
    "    input_shape -- shape of the dataset batch\n",
    "\n",
    "  Returns:\n",
    "    the complete VAE model\n",
    "  \"\"\"\n",
    "  inputs = tf.keras.layers.Input(shape=input_shape)\n",
    "\n",
    "  # get mu, sigma, and z from the encoder output\n",
    "  mu, sigma, z = encoder(inputs)\n",
    "\n",
    "  # get reconstructed output from the decoder\n",
    "  reconstructed = decoder(z)\n",
    "\n",
    "  # define the inputs and outputs of the VAE\n",
    "  model = tf.keras.models.Model(inputs, reconstructed)\n",
    "\n",
    "  # add the KL loss\n",
    "  loss = kl_reconstruction_loss(inputs, z, mu, sigma)\n",
    "  model.add_loss(loss)\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_lbWSKbALf-"
   },
   "source": [
    "다음으로 방금 정의한 인코더, 디코더 및 vae 모델을 반환하는 도우미 함수를 정의하십시오.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hnPo0Pr3nIts",
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def get_models(input_shape, latent_dim):\n",
    "  \"\"\"Returns the encoder, decoder, and vae models\"\"\"\n",
    "  encoder, conv_shape = encoder_model(latent_dim, input_shape)\n",
    "  decoder = decoder_model(latent_dim, conv_shape)\n",
    "  vae = vae_model(encoder, decoder, input_shape) \n",
    "  return encoder, decoder, vae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wJsdzZTPVgOn"
   },
   "source": [
    "Let's use the function above to get the models we need in the training loop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IHdr3CUznL5Z"
   },
   "outputs": [],
   "source": [
    "encoder, decoder, vae = get_models(input_shape=(64,64,3,), latent_dim=LATENT_DIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N6IwN5vlAb5w"
   },
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dHPwSmZFnQ_2"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.002)\n",
    "loss_metric = tf.keras.metrics.Mean()\n",
    "mse_loss = tf.keras.losses.MeanSquaredError()\n",
    "bce_loss = tf.keras.losses.BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWRzFxYkAvXH"
   },
   "source": [
    "4x4 그리드에 16 개의 이미지를 생성하여 이미지 생성 진행률을 표시합니다. 아래에 유틸리티 함수를 정의했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DGe445j0nTmf"
   },
   "outputs": [],
   "source": [
    "def generate_and_save_images(model, epoch, step, test_input):\n",
    "  \"\"\"Helper function to plot our 16 images\n",
    "\n",
    "  Args:\n",
    "\n",
    "  model -- the decoder model\n",
    "  epoch -- current epoch number during training\n",
    "  step -- current step number during training\n",
    "  test_input -- random tensor with shape (16, LATENT_DIM)\n",
    "  \"\"\"\n",
    "  predictions = model.predict(test_input)\n",
    "\n",
    "  fig = plt.figure(figsize=(4,4))\n",
    "\n",
    "  for i in range(predictions.shape[0]):\n",
    "      plt.subplot(4, 4, i+1)\n",
    "      img = predictions[i, :, :, :] * 255\n",
    "      img = img.astype('int32')\n",
    "      plt.imshow(img)\n",
    "      plt.axis('off')\n",
    "\n",
    "  # tight_layout minimizes the overlap between 2 sub-plots\n",
    "  fig.suptitle(\"epoch: {}, step: {}\".format(epoch, step))\n",
    "  plt.savefig('image_at_epoch_{:04d}_step{:04d}.png'.format(epoch, step))\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZgJfazr6A_py"
   },
   "source": [
    "이제 훈련 루프를 시작할 수 있습니다. 에포크 수를 선택하고 가중치 업데이트에 대한 하위 항목을 완료하라는 메시지가 표시됩니다. 일반적인 단계는 다음과 같습니다.\n",
    "\n",
    "* 훈련 배치를 VAE 모델에 공급\n",
    "* reconstruction loss를 계산 합니다 (힌트 : `bce_loss` 대신 위에 정의 된 **mse_loss**를 사용한 다음 이미지의 flattened dimension (예 : 64 x 64 x 3)을 곱합니다.\n",
    "* 총 손실에 KLD 정규화 손실을 추가합니다 (`vae` 모델의 `losses` 속성에 액세스할 수 있음).\n",
    "* get the gradients\n",
    "* use the optimizer to update the weights\n",
    "\n",
    "\n",
    "VAE를 훈련 할 때 얼굴에 많은 변형이 없다는 것을 알 수 있습니다. 그러나 그것이 당신을 방해하지 않도록하십시오! 새 얼굴을 만드는 데 얼마나 잘하는지가 아니라 원래 얼굴을 재구성하는 데 얼마나 잘하는지를 기준으로 테스트합니다.\n",
    "\n",
    "훈련에도 오랜 시간이 걸리며 (30 분 이상) 예상됩니다. 위에 제안 된 평균 손실 측정 항목을 사용한 경우 제출하기 전에 약 320 개로 줄어들 때까지 모델을 학습 시키십시오.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hvL1bHXJnajM"
   },
   "outputs": [],
   "source": [
    "# Training loop. Display generated images each epoch\n",
    "\n",
    "epochs = 50\n",
    "\n",
    "random_vector_for_generation = tf.random.normal(shape=[16, LATENT_DIM])\n",
    "generate_and_save_images(decoder, 0, 0, random_vector_for_generation)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  print('Start of epoch %d' % (epoch,))\n",
    "\n",
    "  # Iterate over the batches of the dataset.\n",
    "  for step, x_batch_train in enumerate(training_dataset):\n",
    "    with tf.GradientTape() as tape:\n",
    "      reconstructed = vae(x_batch_train)\n",
    "\n",
    "      # Compute reconstruction loss\n",
    "      flattened_inputs = tf.reshape(x_batch_train, shape=[-1])\n",
    "      flattened_outputs = tf.reshape(reconstructed, shape=[-1])\n",
    "      loss = mse_loss(flattened_inputs, flattened_outputs) * 12288\n",
    "\n",
    "      # add KLD regularization loss\n",
    "      loss += sum(vae.losses)\n",
    "\n",
    "    # get the gradients and update the weights\n",
    "    grads = tape.gradient(loss, vae.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, vae.trainable_weights))\n",
    "\n",
    "    # compute the loss metric  \n",
    "    loss_metric(loss)\n",
    "\n",
    "    # display outputs every 100 steps\n",
    "    if step % 10 == 0:\n",
    "      display.clear_output(wait=False)    \n",
    "      generate_and_save_images(decoder, epoch, step, random_vector_for_generation)\n",
    "    print('Epoch: %s step: %s mean loss = %s' % (epoch, step, loss_metric.result().numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N5wfzGfABny6"
   },
   "source": [
    "# Plot Reconstructed Images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BnQQlWZHaj90"
   },
   "source": [
    "* 언급했듯이 * 모델은 이미지를 얼마나 잘 재구성 할 수 있는지에 따라 등급이 매겨집니다 (새 이미지를 생성하지 않음). 아래 코드 블록을 통해 어떻게 작동하는지 엿볼 수 있습니다. 테스트 세트에서 배치로 공급하고 입력 (위) 및 출력 (아래) 이미지 행을 플로팅합니다. 출력이 흐릿하더라도 걱정하지 마십시오. 다음과 같이 표시됩니다.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1OPMbZOxX9fx8tK6CGVbrMaQdgyOiQJIC\" width=\"75%\" height=\"60%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TfIbqTIKSXEe"
   },
   "outputs": [],
   "source": [
    "test_dataset = validation_dataset.take(1)\n",
    "output_samples = []\n",
    "\n",
    "for input_image in tfds.as_numpy(test_dataset):\n",
    "      output_samples = input_image\n",
    "\n",
    "idxs = np.random.choice(64, size=10)\n",
    "\n",
    "vae_predicted = vae.predict(test_dataset)\n",
    "display_results(output_samples[idxs], vae_predicted[idxs])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YKUOCA5BtAA"
   },
   "source": [
    "# Plot Generated Images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ylxL9z15ctsy"
   },
   "source": [
    "기본 매개 변수를 사용하면 좋은 가짜 애니메이션 얼굴을 생성 할 수있을만큼 모델을 훈련하는 데 오랜 시간이 걸릴 수 있습니다. 실험하기로 결정한 경우 모델에서 생성 된 가짜 데이터의 8x8 갤러리를 표시하기 위해 아래 코드 블록을 제공했습니다. 다음은 50 epoch 이후 생성 된 샘플 갤러리입니다.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1QwElgfg5TY6vCgI1FK6vdI8Bo6UZKfuX\" width=\"75%\" height=\"60%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zCpTybvGSS6L"
   },
   "outputs": [],
   "source": [
    "def plot_images(rows, cols, images, title):\n",
    "    '''Displays images in a grid.'''\n",
    "    grid = np.zeros(shape=(rows*64, cols*64, 3))\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            grid[row*64:(row+1)*64, col*64:(col+1)*64, :] = images[row*cols + col]\n",
    "\n",
    "    plt.figure(figsize=(12,12))       \n",
    "    plt.imshow(grid)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# initialize random inputs\n",
    "test_vector_for_generation = tf.random.normal(shape=[64, LATENT_DIM])\n",
    "\n",
    "# get predictions from the decoder model\n",
    "predictions= decoder.predict(test_vector_for_generation)\n",
    "\n",
    "# plot the predictions\n",
    "plot_images(8,8,predictions,'Generated Images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4IixoasCfoR"
   },
   "source": [
    "### Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A9E8qwDAVMPs"
   },
   "outputs": [],
   "source": [
    "vae.save(\"anime.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZCJGxjSXJEq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "C4W3_Assignment.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
